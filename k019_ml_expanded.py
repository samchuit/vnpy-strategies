#!/usr/bin/env python3
"""
K019 ML æ‰©å¤§éªŒè¯
æµ‹è¯•æ›´å¤šå“ç§ï¼Œå¢åŠ æ ·æœ¬å¤–éªŒè¯
"""

import numpy as np
import pandas as pd
import os
import json
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from typing import Dict, List, Tuple

DATA_DIR = "/Users/chusungang/workspace/vnpy_strategy/data_minute"
RESULT_DIR = "/Users/chusungang/workspace/vnpy_strategy/result"
os.makedirs(RESULT_DIR, exist_ok=True)

# ===============================
# MLé…ç½®
# ===============================
ML_CONFIG = {
    # æ¨¡å‹å‚æ•°
    "n_estimators": 50,
    "max_depth": 5,
    "test_size": 0.3,  # 30%ä½œä¸ºæµ‹è¯•é›†
    
    # ç‰¹å¾
    "features": ['ma5', 'ma10', 'ma20', 'ma60', 'obv', 'obv_ma5', 'atr', 'vol_ma5'],
    
    # æ ‡ç­¾
    "forward_days": 5,
    "threshold": 0.005,
    
    # éªŒè¯å‚æ•°
    "min_samples": 100,  # æœ€å°‘æ ·æœ¬æ•°
    "min_trades": 5,  # æœ€å°‘äº¤æ˜“æ¬¡æ•°
}


def prepare_features(df: pd.DataFrame) -> pd.DataFrame:
    """å‡†å¤‡ç‰¹å¾"""
    df = df.copy()
    
    # å‡çº¿
    df['ma5'] = df['close'].rolling(5).mean()
    df['ma10'] = df['close'].rolling(10).mean()
    df['ma20'] = df['close'].rolling(20).mean()
    df['ma60'] = df['close'].rolling(60).mean()
    
    # OBV
    obv = [0]
    for i in range(1, len(df)):
        if df['close'].iloc[i] > df['close'].iloc[i-1]:
            obv.append(obv[-1] + df['vol'].iloc[i])
        elif df['close'].iloc[i] < df['close'].iloc[i-1]:
            obv.append(obv[-1] - df['vol'].iloc[i])
        else:
            obv.append(obv[-1])
    df['obv'] = obv
    df['obv_ma5'] = df['obv'].rolling(5).mean()
    
    # ATR
    high_low = df['high'] - df['low']
    high_close = abs(df['high'] - df['close'].shift())
    low_close = abs(df['low'] - df['close'].shift())
    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    df['atr'] = tr.rolling(14).mean()
    
    # æˆäº¤é‡å‡çº¿
    df['vol_ma5'] = df['vol'].rolling(5).mean()
    
    return df.dropna()


def create_labels(df: pd.DataFrame) -> pd.DataFrame:
    """åˆ›å»ºæ ‡ç­¾"""
    df = df.copy()
    
    # æœªæ¥5å¤©æ”¶ç›Šç‡
    df['future_return'] = df['close'].shift(-ML_CONFIG['forward_days']) / df['close'] - 1
    
    # æ ‡ç­¾: 1=ä¸Šæ¶¨, 0=éœ‡è¡/ä¸‹è·Œ
    df['label'] = (df['future_return'] > ML_CONFIG['threshold']).astype(int)
    
    return df.dropna()


def train_and_test(df: pd.DataFrame, symbol: str) -> Dict:
    """è®­ç»ƒå¹¶æµ‹è¯•æ¨¡å‹"""
    # å‡†å¤‡æ•°æ®
    df = prepare_features(df)
    df = create_labels(df)
    
    if len(df) < ML_CONFIG['min_samples']:
        return None
    
    features = ML_CONFIG['features']
    X = df[features].values
    y = df['label'].values
    
    # æ—¶åºåˆ†å‰²
    split = int(len(X) * (1 - ML_CONFIG['test_size']))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è®­ç»ƒæ¨¡å‹
    model = RandomForestClassifier(
        n_estimators=ML_CONFIG['n_estimators'],
        max_depth=ML_CONFIG['max_depth'],
        random_state=42
    )
    model.fit(X_train_scaled, y_train)
    
    # æ¨¡å‹è¯„ä¼°
    train_score = model.score(X_train_scaled, y_train)
    test_score = model.score(X_test_scaled, y_test)
    
    # å›æµ‹
    position = 0
    entry_price = 0
    returns = []
    
    for i in range(len(X_test_scaled)):
        if i >= len(y_test):
            break
        
        pred = model.predict([X_test_scaled[i]])[0]
        close_price = df['close'].iloc[split + i]
        
        if position == 0 and pred == 1:
            position = 1
            entry_price = close_price
        
        elif position == 1:
            ret = (close_price - entry_price) / entry_price
            # 5%æ­¢æŸ
            if abs(ret) > 0.05:
                returns.append(ret)
                position = 0
    
    if len(returns) < ML_CONFIG['min_trades']:
        return None
    
    returns = np.array(returns)
    total_ret = (1 + returns).prod() - 1
    
    if len(returns) > 1 and returns.std() > 0:
        sharpe = returns.mean() / returns.std() * np.sqrt(252 * 4)
    else:
        sharpe = 0
    
    win_rate = (returns > 0).mean()
    
    return {
        'symbol': symbol,
        'train_accuracy': train_score,
        'test_accuracy': test_score,
        'total_return': total_ret,
        'sharpe': sharpe,
        'win_rate': win_rate,
        'trades': len(returns),
        'overfitting_risk': train_score - test_score  # è¿‡æ‹Ÿåˆé£é™©
    }


def run_expanded_validation():
    """è¿è¡Œæ‰©å¤§éªŒè¯"""
    print("\n" + "=" * 70)
    print("ğŸ”¬ K019 ML æ‰©å¤§éªŒè¯")
    print("=" * 70)
    
    # è·å–æ‰€æœ‰60åˆ†é’Ÿæ•°æ®
    all_files = [f.replace('_60.csv', '') for f in os.listdir(DATA_DIR) if f.endswith('_60.csv')]
    
    print(f"\nğŸ“‚ å‘ç° {len(all_files)} ä¸ªå“ç§æ•°æ®")
    
    # åŠ è½½å¹¶éªŒè¯
    results = []
    failed = []
    
    for symbol in all_files:
        file_path = f"{DATA_DIR}/{symbol}_60.csv"
        
        try:
            df = pd.read_csv(file_path)
            result = train_and_test(df, symbol)
            
            if result:
                results.append(result)
                print(f"   âœ… {symbol}: å¤æ™®={result['sharpe']:.2f}, æ”¶ç›Š={result['total_return']*100:.1f}%, æµ‹è¯•å‡†ç¡®ç‡={result['test_accuracy']:.1%}")
            else:
                failed.append(symbol)
                print(f"   âŒ {symbol}: æ ·æœ¬ä¸è¶³")
                
        except Exception as e:
            failed.append(symbol)
            print(f"   âŒ {symbol}: {str(e)[:50]}")
    
    # ç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š éªŒè¯ç»“æœç»Ÿè®¡")
    print("=" * 70)
    
    if results:
        positive = sum(1 for r in results if r['total_return'] > 0)
        positive_sharpe = sum(1 for r in results if r['sharpe'] > 0)
        avg_sharpe = sum(r['sharpe'] for r in results) / len(results)
        avg_return = sum(r['total_return'] for r in results) / len(results)
        avg_overfit = sum(r['overfitting_risk'] for r in results) / len(results)
        
        print(f"\næ€»æµ‹è¯•å“ç§: {len(results)}")
        print(f"æ­£æ”¶ç›Š: {positive}/{len(results)} ({positive/len(results)*100:.1f}%)")
        print(f"æ­£å¤æ™®: {positive_sharpe}/{len(results)} ({positive_sharpe/len(results)*100:.1f}%)")
        print(f"å¹³å‡æ”¶ç›Š: {avg_return*100:.1f}%")
        print(f"å¹³å‡å¤æ™®: {avg_sharpe:.3f}")
        print(f"å¹³å‡è¿‡æ‹Ÿåˆé£é™©: {avg_overfit:.1%}")
        
        # é£é™©è¯„ä¼°
        print(f"\nâš ï¸ é£é™©è¯„ä¼°:")
        if avg_overfit > 0.15:
            print(f"   é«˜è¿‡æ‹Ÿåˆé£é™© ({avg_overfit:.1%})")
            print(f"   å»ºè®®: å¢åŠ æ­£åˆ™åŒ–æˆ–å‡å°‘æ¨¡å‹å¤æ‚åº¦")
        elif avg_overfit > 0.05:
            print(f"   ä¸­ç­‰è¿‡æ‹Ÿåˆé£é™© ({avg_overfit:.1%})")
            print(f"   å»ºè®®: æ¨¡å‹è¡¨ç°æ­£å¸¸ï¼Œä½†éœ€å…³æ³¨")
        else:
            print(f"   ä½è¿‡æ‹Ÿåˆé£é™© ({avg_overfit:.1%}) âœ…")
            print(f"   å»ºè®®: æ¨¡å‹è¡¨ç°ç¨³å¥")
        
        # Top 5
        print(f"\nğŸ† Top 5 å“ç§:")
        top5 = sorted(results, key=lambda x: x['sharpe'], reverse=True)[:5]
        for i, r in enumerate(top5, 1):
            risk = "âš ï¸" if r['overfitting_risk'] > 0.15 else "âœ…"
            print(f"   {i}. {r['symbol']}: å¤æ™®={r['sharpe']:.2f}, æ”¶ç›Š={r['total_return']*100:.1f}% {risk}")
        
        # é¿å…å“ç§
        print(f"\nğŸ›‘ éœ€éªŒè¯å“ç§:")
        bottom3 = sorted(results, key=lambda x: x['sharpe'])[:3]
        for r in bottom3:
            if r['sharpe'] < 0:
                print(f"   âŒ {r['symbol']}: å¤æ™®={r['sharpe']:.2f}")
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = f"{RESULT_DIR}/k019_ml_expanded_{timestamp}.json"
        
        with open(result_file, 'w') as f:
            json.dump({
                'config': ML_CONFIG,
                'results': results,
                'summary': {
                    'total': len(results),
                    'positive': positive,
                    'positive_sharpe': positive_sharpe,
                    'avg_sharpe': avg_sharpe,
                    'avg_return': avg_return,
                    'avg_overfitting_risk': avg_overfit,
                    'failed': failed
                }
            }, f, indent=2, default=str)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {result_file}")
        
    else:
        print("\nâŒ æ²¡æœ‰æœ‰æ•ˆçš„éªŒè¯ç»“æœ")
    
    return results


def analyze_feature_importance():
    """åˆ†æç‰¹å¾é‡è¦æ€§"""
    print("\n" + "=" * 70)
    print("ğŸ“ˆ ç‰¹å¾é‡è¦æ€§åˆ†æ")
    print("=" * 70)
    
    all_files = [f.replace('_60.csv', '') for f in os.listdir(DATA_DIR) if f.endswith('_60.csv')]
    
    feature_importance = {f: 0 for f in ML_CONFIG['features']}
    count = 0
    
    for symbol in all_files[:10]:  # åˆ†æå‰10ä¸ª
        file_path = f"{DATA_DIR}/{symbol}_60.csv"
        
        try:
            df = pd.read_csv(file_path)
            df = prepare_features(df)
            df = create_labels(df)
            
            if len(df) < ML_CONFIG['min_samples']:
                continue
            
            features = ML_CONFIG['features']
            X = df[features].values
            y = df['label'].values
            
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            model = RandomForestClassifier(
                n_estimators=ML_CONFIG['n_estimators'],
                max_depth=ML_CONFIG['max_depth'],
                random_state=42
            )
            model.fit(X_scaled, y)
            
            for i, f in enumerate(features):
                feature_importance[f] += model.feature_importances_[i]
            
            count += 1
            
        except Exception as e:
            continue
    
    if count > 0:
        print(f"\nåˆ†æ {count} ä¸ªå“ç§...")
        print(f"\nç‰¹å¾é‡è¦æ€§æ’å:")
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        for i, (feat, imp) in enumerate(sorted_features, 1):
            bar = "â–ˆ" * int(imp * 50)
            print(f"   {i}. {feat:12s}: {imp:.3f} {bar}")


def main():
    """ä¸»ç¨‹åº"""
    print("=" * 70)
    print("ğŸ”¬ K019 ML æ‰©å¤§éªŒè¯")
    print("=" * 70)
    
    print(f"\nğŸ“‹ éªŒè¯å†…å®¹:")
    print(f"   1. æµ‹è¯•æ‰€æœ‰60åˆ†é’Ÿæ•°æ® ({len(os.listdir(DATA_DIR))} ä¸ªå“ç§)")
    print(f"   2. 70%è®­ç»ƒ / 30%æµ‹è¯•")
    print(f"   3. æ—¶åºäº¤å‰éªŒè¯")
    print(f"   4. è¿‡æ‹Ÿåˆé£é™©è¯„ä¼°")
    
    # è¿è¡ŒéªŒè¯
    results = run_expanded_validation()
    
    # ç‰¹å¾é‡è¦æ€§
    analyze_feature_importance()
    
    return results


if __name__ == "__main__":
    main()
